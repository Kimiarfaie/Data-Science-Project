---
title: "Project"
author: "Seyedeh Kimia Arfaie Oghani"
output: html_document
date: "2024-05-20"
---

```{r Loading Packages}
library(readr)
library(MASS)
library(caret)
library(dplyr)
library(e1071)
```

```{r Data Loading}
# Read the dataset
data <- read_csv("dataset.csv",show_col_types = FALSE)
head(data)
```

```{r Data preprocessing}
# Rename 'Nacionality' to 'Nationality'
names(data)[names(data) == "Nacionality"] <- "Nationality"

# Check for null values in each column
null_values <- sapply(data, function(x) sum(is.na(x)))

# Print out null value counts
cat("Null Values in Each Column:\n")
cat(paste(names(null_values), null_values, sep=": ", collapse="\n"), "\n")

# Examine categories in the target column
cat("Categories in the Target Column:\n")
print(unique(data$Target))

# Encode the categories as integers
data$Target <- factor(data$Target, levels = c("Dropout", "Enrolled", "Graduate"))
data$Target <- as.integer(data$Target) - 1  # Subtract 1 to start encoding from 0

# Check the transformation
print(table(data$Target))
```


```{r Correlation Analysis}
# Compute correlation matrix
cor_matrix <- cor(data, use = "complete.obs")  # Handles missing values by using complete cases
target_correlations <- cor_matrix["Target", ]

# Sort the correlations in descending order
sorted_target_correlations <- sort(target_correlations, decreasing = TRUE)

# Print the sorted correlations with 'Target'
print(sorted_target_correlations)

```
 

```{r Spearman's Rank Correlation}
# Compute Spearman's correlation matrix
spearman_cor_matrix <- cor(data, method = "spearman", use = "complete.obs")

# Extract the correlations with the 'Target' column
target_spearman_correlations <- spearman_cor_matrix["Target", ]

# Sort the correlations in descending order
sorted_target_spearman_corr <- sort(target_spearman_correlations, decreasing = TRUE)

# Print Spearman's Rank Correlation with the 'Target'
cat("Spearman's Rank Correlation with Target:\n")
print(sorted_target_spearman_corr)

```

Cleaning the Dataset

```{r Identifying features and cleaning the dataset}

# Convert these vectors to data frames
df_pearson <- data.frame(Feature = names(target_correlations), PearsonCorrelation = unname(target_correlations))
df_spearman <- data.frame(Feature = names(target_spearman_correlations), SpearmanCorrelation = unname(target_spearman_correlations))

# Merge both data frames by feature names
correlations_combined <- merge(df_pearson, df_spearman, by = "Feature")

# Filter features where both correlations are between -0.1 and 0.1
weakly_correlated_features <- correlations_combined %>%
  filter(abs(PearsonCorrelation) < 0.04 & abs(SpearmanCorrelation) < 0.04)

# Print the features that meet this criteria
print(weakly_correlated_features)


```
Removing Nationality, Educational special needs, Inflation rate, International, Unemployment rate, Mother's qualification, Father's qualification from data as they have significantly low correlation with Target variables in both correlations. 

```{r Data cleaning}

# Remove the specified columns
data <- data %>%
  dplyr::select(-`Nationality`, -`Educational special needs`, -`Inflation rate`, -`International`, -`Unemployment rate`, -`Mother's qualification`, -`Father's qualification`)
```

```{r Visualization of Data distribution}
# Counting each of the classes
target_counts <- table(data$Target)

# Print the counts
print(target_counts)

# Basic Pie Chart with Base R
pie(target_counts, main = "Pie Chart of Target Variable", col = rainbow(length(target_counts)))


```
We remove the "Enrolled" class (encoded as 1), and then re-encode "Graduate"  as 1 and "Dropout" as 0 in our dataset.

```{r Removing Enrolled class}

# Filter out 'Enrolled' class
data <- filter(data, Target != 1)

# Step 2: Re-encode 'Graduate' from 2 to 1
data$Target <- ifelse(data$Target == 2, 1, 0)

# Check the new distribution of the Target variable to confirm changes
table(data$Target)

```


```{r Target Distribution by Sex, age, marriage}
# Distribution by Sex
# Create the crosstab for Target and Gender
ct_gender <- table(data$Target, data$Gender)

# Rename the rows and columns to make the table more readable
dimnames(ct_gender) <- list(Target = c("Dropout", "Graduate"),
                            Gender = c("Female", "Male"))

# Print the crosstab
print(ct_gender)

# Convert the table to a data frame for plotting
df_gender <- as.data.frame(ct_gender)

# Rename the columns for clarity if needed
names(df_gender) <- c("Target", "Gender", "Count")

# Create the bar plot
ggplot(df_gender, aes(x = Gender, y = Count, fill = Target)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Distribution of Graduates and Dropouts by Gender",
       x = "Gender",
       y = "Count") +
  scale_fill_brewer(palette = "Pastel1", labels = c("Dropout", "Graduate")) +
  theme_minimal()

# Distribution by Age
# Create a histogram of Age at Enrollment
ggplot(data, aes(x = `Age at enrollment`)) +
  geom_histogram(binwidth = 1, fill = "dodgerblue", alpha = 0.5) +
  labs(title = "Distribution by Age", 
       x = "Age at Enrollment", 
       y = "Total Students") +
  theme(plot.title = element_text(hjust = 0.5))  # Center the title

# Distribution by Martial Status
# Create a count plot of Marital Status with hue by Target
ggplot(data, aes(x = factor(`Marital status`, levels = c(1, 2, 3, 4, 5,6),
                           labels = c('Single', 'Married', 'Widower', 'Divorced', 'Defacto union', 'Legally separated')), fill = factor(Target))) +
  geom_bar(stat = "count", position = position_dodge()) +
  scale_fill_manual(values = c("#FF9999", "#9999FF"), 
                    labels = c("0 (Dropout)", "1 (Graduate)"), 
                    name = "Target") +
  labs(title = "Distribution by Marital Status",
       x = "Marital Status",
       y = "Total Students") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```

1. KNN method

```{r KNN}
# Scale the data
data_scaled <- scale(data[, -which(names(data) == "Target")])  # Exclude the target variable

# Convert scaled data back to a data frame and reattach the target variable
data_scaled <- as.data.frame(data_scaled)
data_scaled$Target <- data$Target

# Splitting The data
set.seed(123) 
trainIndex <- createDataPartition(data_scaled$Target, p = .75, 
                                  list = FALSE, 
                                  times = 1)
trainData <- data_scaled[trainIndex, ]
testData <- data_scaled[-trainIndex, ]

# Convert Target to a factor for classification
trainData$Target <- factor(trainData$Target, levels = c(0, 1), labels = c("Dropout", "Graduate"))
testData$Target <- factor(testData$Target, levels = c(0, 1), labels = c("Dropout", "Graduate"))

# KNN training , Cross validation included to ge the optimal k

# Define training control
train_control <- trainControl(
  method = "cv",    # Cross-validation
  number = 10       # Number of folds
)

# Train the model with a range of k values to find the optimal
mknn <- train(
  Target ~ ., 
  data = trainData, 
  method = "knn",
  trControl = train_control,
  tuneLength = 20   # Tune over 20 different values of k
)

# Print the results to see the best 'k'
print(mknn)
plot(mknn)

# Using the best k value found, predict on test data
best_k <- mknn$bestTune$k
mknn <- knn(train = trainData[, -which(names(trainData) == "Target")], 
                test = testData[, -which(names(testData) == "Target")],
                cl = trainData$Target, k = best_k)

# Evaluate the model
conf_matrix <- table(Predicted = mknn, Actual = testData$Target)
print(conf_matrix)

# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("Accuracy:", accuracy))

```
2. LDA

```{r LDA}

# Set seed for reproducibility
set.seed(123)

# Ensuring Target is a factor
data$Target <- factor(data$Target, levels = c(0, 1), labels = c("Dropout", "Graduate"))

# Splitting the data
index <- createDataPartition(y = data$Target, p = 0.75, list = TRUE, times = 1)
trainData <- data[index[[1]], ]
testData <- data[-index[[1]], ]

# Fitting LDA model on training data using MASS
mlda <- lda(Target ~ ., data = trainData)

# Summary of the model
print(summary(mlda))

# Predictions on the testing set
predictions <- predict(mlda, testData)
predictedClasses <- predictions$class

# Confusion Matrix and accuracy
conf_matrix <- table(Predicted = predictedClasses, Actual = testData$Target)
print(conf_matrix)

# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy:", accuracy, "\n")


```
3. Logistic Regression

```{r Logistic Regression}
# Set seed for reproducibility
set.seed(123)

# Splitting the data
index <- createDataPartition(y = data$Target, p = 0.75, list = TRUE, times = 1)
trainData <- data[index[[1]], ]
testData <- data[-index[[1]], ]

# Fit the logistic regression model - Full model with all the predictors
mlr <- glm(Target ~ ., data = trainData, family = binomial())

# Perform stepwise selection to select most significant variables
mlr <- step(mlr, direction="both", trace=0)

# Summary of the selected model
summary(mlr)

# Evaluate using cross-validation manually
cv_results <- train(mlr$formula, data = trainData, method = "glm", 
                    family = binomial(), trControl = trainControl(method = "cv", number = 10, classProbs = TRUE))

# Print cross-validation results
print(cv_results)

# Predict and evaluate on the test data
predicted_probs <- predict(mlr, newdata = testData, type = "response")
predicted_classes <- ifelse(predicted_probs > 0.5, "Graduate", "Dropout")
conf_matrix <- table(Predicted = predicted_classes, Actual = testData$Target)
print(conf_matrix)

# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Accuracy:", accuracy, "\n")

```
4. Decision Trees

```{r Decision Trees}
library(rpart)
library(rpart.plot)

mtree <- rpart(Target ~ ., data = trainData, method = "class")

# Print the summary of the tree
print(summary(mtree))

# Plot the decision tree
rpart.plot(mtree)

# Predict using the decision tree
predicted_classes <- predict(mtree, testData, type = "class")

# Create a confusion matrix to evaluate predictions
conf_matrix <- table(Predicted = predicted_classes, Actual = testData$Target)
print(conf_matrix)

# Calculate accuracy
cat("\n-------\n")
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("\nAccuracy:", accuracy, "\n")



```

5. Random Forests

```{r}
# Set seed for reproducibility
set.seed(123)

# Set up training control for cross validation

train_control <- trainControl(
  method = "cv",       # k-fold cross-validation
  number = 5,          # number of folds
  savePredictions = "final",
)

# Define the tuning grid for mtry
num_features <- ncol(trainData) - 1
mtry_values <- seq(from = sqrt(num_features), to = num_features/2, length.out = 5)
tune_grid <- expand.grid(mtry = round(mtry_values))

# Train the model named 'mrf' with the tuning grid
mrf <- train(Target ~ ., data = trainData, method = "rf",
             trControl = train_control, tuneGrid = tune_grid,
             metric = "Accuracy")

# Output the results
print(mrf)

# Make predictions on the test dataset
test_predictions <- predict(mrf, newdata = testData)

# Create a confusion matrix to evaluate predictions
conf_matrix <- table(Predicted = test_predictions, Actual = testData$Target)
print(conf_matrix)

# Calculate accuracy
test_accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("-------------\n")
cat("Test Set Accuracy: ", test_accuracy, "\n")


```

6. ANN

```{r ANN}
# Load necessary libraries
library(nnet)
set.seed(1)

data_copy <- data

index <- createDataPartition(y = data_copy$Target, p = 0.75, list = TRUE, times = 1)
trainData <- data_copy[index[[1]], ]
testData <- data_copy[-index[[1]], ]

sizes <- seq(5, 10)

results <- data.frame(size = integer(), accuracy = numeric(), stringsAsFactors = FALSE)


# Loop over sizes
for (size in sizes) {
    # Train model with current size
    set.seed(123)  # for reproducibility
    model <- nnet(Target ~ ., data = trainData, size = size, decay = 0.1, maxit = 500)

    # Predict on the training set (or validation set if available)
    predictions <- predict(model, newdata = trainData, type = "class")
    
    # Create a confusion matrix and calculate accuracy
    conf_matrix <- table(Predicted = predictions, Actual = trainData$Target)
    accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
    
    # Store results
    results <- rbind(results, data.frame(size = size, accuracy = accuracy))
}

# Print the results
print(results)

```
```{r Best ANN model}
set.seed(123)
best_size <- 6
mann <- nnet(Target ~ ., data = trainData, size = best_size, decay = 0.1, maxit = 500)
# Predict on the training set (or validation set if available)
predictions <- predict(model, newdata = trainData, type = "class")
    
# Create a confusion matrix and calculate accuracy
conf_matrix <- table(Predicted = predictions, Actual = trainData$Target)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
accuracy
```

7. SVM

```{r Getting the best cost}
set.seed(123)  # For reproducibility
indexes <- sample(1:nrow(data), size = 0.75 * nrow(data))
trainData <- data[indexes, ]
testData <- data[-indexes, ]

cost_values <- 10^seq(-1, 1, by = 1)  # From 0.001 to 10 in exponential steps

tuning_results <- tune.svm(x = trainData[, -which(names(trainData) == "Target")],
                           y = trainData$Target,
                           kernel = "linear",
                           cost = cost_values,
                           scale = TRUE)
cat("Best Cost:", tuning_results$best.parameters$cost, "\n")
```

```{r SVM}
set.seed(123)  # For reproducibility
indexes <- sample(1:nrow(data), size = 0.75 * nrow(data))
trainData <- data[indexes, ]
testData <- data[-indexes, ]

msvm <- svm(Target ~ ., data = trainData, kernel = "linear", cost = 10, scale = TRUE)

# Predict on test data
predictions <- predict(msvm, newdata = testData)

# Create a confusion matrix to evaluate predictions
conf_matrix <- table(Predictions = predictions, Actual = testData$Target)
print(conf_matrix)

# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
cat("Test Set Accuracy: ", accuracy, "\n")

```
8. Clustering technique

```{r Kmeans Clustering}
# Load necessary library
library(tidyverse)

# Scale the data
data_scaled <- scale(data[, -which(names(data) == "Target")])  # Exclude the target variable

# Convert scaled data back to a data frame and reattach the target variable
data_scaled <- as.data.frame(data_scaled)
data_scaled$Target <- data$Target

# Remove rows with NA values
data_scaled <- data_scaled[complete.cases(data_scaled), ]

# Splitting the data
set.seed(123) 
trainIndex <- createDataPartition(data_scaled$Target, p = .75, 
                                  list = FALSE, 
                                  times = 1)
trainData <- data_scaled[trainIndex, ]
testData <- data_scaled[-trainIndex, ]

# Set the number of clusters to 2 based on the classification problem
k <- 2

# Train the K-Means model with the optimal number of clusters
mkmc <- kmeans(trainData[, -which(names(trainData) == "Target")], centers = k, nstart = 20)

clusters <- mkmc$cluster

# Map clusters to class labels using the training data
map_clusters <- function(clusters, actual) {
  mapping <- sapply(unique(clusters), function(cluster) {
    mode <- which.max(table(actual[clusters == cluster]))
    return(mode)
  })
  return(mapping[clusters])
}

# Create the mapping based on the training data
cluster_mapping <- map_clusters(clusters, trainData$Target)
predicted_labels <- cluster_mapping

# Create a confusion matrix to evaluate predictions
conf_matrix <- table(Predicted = predicted_labels, Actual = trainData$Target)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)

# Print the results
print(conf_matrix)
cat("Accuracy:", accuracy, "\n")

```
```{r Final Models}
save(mknn, mlda, mlr, mtree, mrf, mann, msvm, mkmc, file = "final_models.RData")
```

